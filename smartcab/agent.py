import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator

OPTIMIZED = True


class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        # Set the agent in the evironment
        super(LearningAgent, self).__init__(env)
        # Create a route planner
        self.planner = RoutePlanner(self.env, self)
        # The set of valid actions
        self.valid_actions = self.env.valid_actions

        # Set parameters of the learning agent
        self.learning = learning    # Whether the agent is expected to learn
        self.Q = dict()             # Create a Q-table
        self.epsilon = epsilon      # Random exploration factor
        self.alpha = alpha          # Learning factor

        self.trial = 0
        self.testing = False

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)

        self.testing = testing
        if testing:
            self.epsilon = 0.0
            self.alpha = 0.0
        else:
            if not OPTIMIZED:
                if self.trial == 0:
                    return
                self.epsilon = self.epsilon - 0.05
            else:
                a = 0.005
                self.epsilon = math.cos(a * self.trial)

        self.trial += 1

    def build_state(self):
        """ The build_state function is called when the agent requests data
        from the environment. The next waypoint, the intersection inputs, and
        the deadline are all features available to the agent. """

        # Collect data about the environment
        waypoint = self.planner.next_waypoint()
        inputs = self.env.sense(self)

        # The key insight here is the following: the agent doesn't really have
        # to learn all traffic rules. As long as it isn't involved in major
        # accidents, it will probably be fine.

        # This smaller state increases the probability that all states will be
        # visited, giving the agent the chance to learn something useful
        state = (
            waypoint,
            inputs['light'],
            inputs['oncoming'],
        )

        return state

    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find
        the maximum Q-value of all actions based on the 'state' the smartcab is
        in. """

        return max(self.Q[state].values())

    def createQ(self, state):
        """ The createQ function is called when a state is generated by the
        agent. """

        if self.learning:
            if state not in self.Q:
                self.Q[state] = {a: 0.0 for a in self.valid_actions}

    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        self.state = state
        self.next_waypoint = self.planner.next_waypoint()

        if not self.learning:
                return random.choice(self.valid_actions)

        if random.random() < self.epsilon:
            return random.choice(self.valid_actions)
        else:
            maxQ = self.get_maxQ(state)
            actions = [k for k, v in self.Q[state].items() if v == maxQ]
            random.shuffle(actions)
            return actions[0]

    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards
            when conducting learning. """

        if not self.learning:
            return

        self.Q[state][action] = (1-self.alpha)*self.Q[state][action] + \
            self.alpha*reward

    def update(self):
        """ The update function is called when a time step is completed in the
        environment for a given trial. This function will build the agent
        state, choose an action, receive a reward, and learn if enabled. """

        # Get current state
        state = self.build_state()
        # Create 'state' in Q-table
        self.createQ(state)
        # Choose an action
        action = self.choose_action(state)
        # Receive a reward
        reward = self.env.act(self, action)
        # Q-learn
        self.learn(state, action, reward)


def run():
    """ Driving function for running the simulation.  Press ESC to close the
    simulation, or [SPACE] to pause the simulation. """

    if not OPTIMIZED:
        env = Environment(verbose=False)
        agent = env.create_agent(LearningAgent, learning=True, epsilon=1.0,
                                 alpha=0.5)
        env.set_primary_agent(agent, enforce_deadline=True)
        sim = Simulator(env, update_delay=0, log_metrics=True,
                        optimized=False, display=False)
        sim.run(tolerance=0.05, n_test=10)
    else:
        ##############
        # Create the environment
        # Flags:
        #   verbose     - set to True to display additional output from the
        #                 simulation
        #   num_dummies - discrete number of dummy agents in the environment,
        #                 default is 100
        #   grid_size   - discrete number of intersections (columns, rows),
        #                 default is (8, 6)
        env = Environment()

        ##############
        # Create the driving agent
        # Flags:
        #   learning   - set to True to force the driving agent to use
        #                Q-learning
        #    * epsilon - continuous value for the exploration factor, default
        #                is 1
        #    * alpha   - continuous value for the learning rate, default is 0.5
        agent = env.create_agent(LearningAgent, learning=True, epsilon=1.0,
                                 alpha=0.5)

        ##############
        # Follow the driving agent
        # Flags:
        #   enforce_deadline - set to True to enforce a deadline metric
        env.set_primary_agent(agent, enforce_deadline=True)

        ##############
        # Create the simulation
        # Flags:
        #   update_delay - continuous time (in seconds) between actions,
        #                  default is 2.0 seconds
        #   display      - set to False to disable the GUI if PyGame is enabled
        #   log_metrics  - set to True to log trial and simulation results to
        #                  /logs
        #   optimized    - set to True to change the default log file name
        sim = Simulator(env, update_delay=0, log_metrics=True, optimized=True,
                        display=False)

        ##############
        # Run the simulator
        # Flags:
        #   tolerance  - epsilon tolerance before beginning testing, default is
        #                0.05
        #   n_test     - discrete number of testing trials to perform, default
        #                is 0
        sim.run(tolerance=0.03, n_test=10)


if __name__ == '__main__':
    run()
